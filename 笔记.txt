一、"大数据时代"，数据获取的方式：

    1、企业生产的用户数据：大型互联网公司有海量用户，所以他们积累数据有天然的优势
                        有数据意识的中小型企业，也开始积累的数据。

    2、数据管理咨询公司：通常这样的公司有很庞大的数据采集团队，一般会通过市场调研、问卷调查、固定的样本检测
                        和各行各业的公司进行合作、专家对话（数据积累了很多年了，最后得出科研结果）来采集数据。

    3、政府/机构提供的公开数据：政府通过各地政府统计上报的数据进行合并；机构都是权威的第三方网站。

    4、第三方数据平台购买数据：通过各个数据交易平台来购买各行各业需要的数据，根据获取难度不同，价格也会不同

    5、爬虫爬出数据：如果市场是没有我们需要的数据，或者价格太高不愿意买，那么就可以招/做一个爬出工程师，从互联网定向采集数据。


二、什么是爬虫？

    爬虫：就是抓取网页数据的程序。


三、爬虫怎么抓取网页数据：

    网页三大特征:
        1、每个网页都有自己的URL（统一资源定位符）来进行定位
        2、网页都使用HTML（超文本标记语言）来描述页面信息。
        3、网页都是用HTTP/HTTPS(超文本传输协议)来传输HTML数据。

    爬虫的设计思路：
        1、首先确定需要爬取的网页URL地址。
        2、通过HTTP/HTTP协议来获取对应的HTML页面。
        3、提取HTML页面里有用的数据：
            a.如果是需要的数据，就保存起来。
            b.如果是页面里的其他URL，那就继续执行第二步。


四、为什么选择Python做爬虫？

    可以做爬虫的语言有很多，如PHP、Java、C/C++、Python等等...

        PHP 虽然是世界上最好的语言，但是它天生不是干这个的，而且对多线程、异步支持不够好，并发处理能力很弱
                爬虫是工具性：程序，对速度和效率要求比较高。

        Java 的网络爬虫生态圈也很完善，是Python爬虫最大的对手。但是Java语言本身很笨重，代码量很大。
                重构成本比较高，任何修改都会导致代码的大量变动，爬虫经常需要修改部分采集代码。

        C/C++ 运行效率和性能几乎最强，但是学习成本很高，代码成型比较慢。
                能用C/C++做爬虫，只能说是能力的表现，但是不是正确的选择。

        Python 语法优美、代码简洁、开发效率高、支持的模块、相关的HTTP请求模块丰富。
               还有强大的Scrapy、以及成熟高效的 Scrapy-redis分布式策略
               而且，调用其他接口也非常方便（胶水语言）


五、课程介绍：

    1、Python的基本语法知识

    2、如何抓取HTML页面：
            HTTP请求的处理, urllib、urllib2、requests
            处理后的请求可以模拟浏览器发送请求，获取服务器响应的文件

    3、解析服务器响应的内容
            re、xpath、beautifulSoup4(bs4)、jsonpath、pyquery等
            使用某种描述性一样来给我们需要提取的数据定义一个匹配规则，
            符合这个规则的数据就会被匹配

    4、如何采集动态HTML、验证码的处理
            通用的动态页面采集：Selenium+PhantomJS(无界面)：模拟真实浏览器加载js、ajax等非静态的页面数据。
            Tesseract：机器学习库，机器图像识别系统，可以处理简单的验证码，复杂的验证码可以通过手动输入/专门的打码平台

    5、Scrapy框架：（Scrapy，Pyspider）
            高定制性、高性能（异步网络框架twisted），所以数据下载速度非常快，提供了数据存储、数据下载、提取规则等组件。

    6、分布式策略 Scrapy-Redis：
            Scrapy-Redis，在Scrapy的基础上添加了一套以Redis数据库位核心的一套组件
                让Scrapy框架框架支持分布式的功能，主要在Redis里做请求指纹去重、请求分配、数据临时存储。

    7 爬虫 ... 反爬虫 ... 反反爬虫之间的斗争：
            其实爬虫做到最后，最头疼的不是复杂的页面，也不是晦涩的数据，而是网站另一边的反爬虫人员。

            User-Agent、代理、验证码、动态数据加载、加密数据

            数据价值是否值得去费劲做反爬虫

            1、机器成本 + 人力成本 > 数据价值，就不反了，一般做到封IP就结束了

            2、面子的战争...

            爬虫和反爬虫之间的斗争，最后一定是爬虫获胜！
                为什么？只要是真实用户可以浏览的网页数据、爬虫就一定能爬下来！


六、根据使用场景：分为通用爬虫..聚焦爬虫

    1、通用爬虫：搜索引擎用的爬虫系统
        1、目标：就是尽可能把互联网上所有的网页下载下来，放到本地服务器里形成备份再对这些网页做相关处理
            （提取关键字、去掉广告），最后提供一个用户检索接口

        2、抓取流程：
            a）首先选择一部分已有的URL，把这些URL放到待爬取队列
            b）从队列里取出这些URL，然后解析DNS得到主机IP，然后去这个IP对应的服务器里下载HTML页面，保存到搜索引擎的本地服务器里
                之后吧这些爬过的URL放入已爬取队列。
            c）分析这些网页内容，找出网页里其他的URL连接，继续执行第二步，直到爬取条件结束

        3、搜索引擎如何获取一个新网站的URL？
            1、主动向搜索引擎提交网址
            2、在其他网站里设置网站的外链
            3、搜索引擎会和DNS服务商进行合作，可以快速收录新的网站。

            DNS：就是把域名解析成IP的一种技术。


        4、通用爬虫并不是万物皆可爬，它也要遵守规则：
            Robots协议：协议会指明通用爬虫可以爬取网页的权限。
            Robots.txt：只是一个建议。并不是所有爬虫都遵守，一般只有大型的搜索引擎爬出才会遵守。
                   咱们个人写的爬虫，就不用管。

        5、通用爬虫工作流程：爬取网页 --- 存储数据 --- 内容处理 --- 提供检索/排名服务

        6、搜索引擎排名：
            1、PageRank值：根据网站的流量（点击量/浏览量/人气）统计，流量越高，网站也越值钱，排名也越高。
            2、竞价排名：谁给钱多，谁排名就高。

        7、通用爬虫的缺点：
            1.只能提供和文本相关的内容（HTML、Word、PDF）等等，但是不能提供多媒体（音乐、图片、视频）和二进制文件（程序、脚本）等等
            2.提供的结果千篇一律，不能针对不同背景领域的人提供不同的搜索结果。
            3.不能理解人类语义上的检索。


    聚焦爬虫：爬虫程序员写的针对某种内容爬虫

    面向主题爬虫，面向需求爬虫，会针对某种特定的内容去爬取信息，而且会保证信息和需求尽可能相关。

    Python自带的模块：/usr/lib/python2.7/urllib2.py
    Python 第三方模块：/usr/local/lib/python2.7/site-packages


    urllib2 默认的 User-Agent：Python-urllib/2.7

    mono --arch=32 Fiddler.exe

    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36"

    User-Agent 是爬虫和反爬虫斗争的第一步，养成好习惯，发送请求带User-Agent


    urllib 的urlencode() 接受的参数是一个字典：
    wd = {"wd":"传智播客"}
    urllib.urlencode(wd)
    结果：wd=%E4%BC%A0%E6%99%BA%E6%92%AD%E5%AE%A2


    Get 和 Post 请求的区别：

    Get：请求的url会附带查询参数
    Post：请求的url不带参数

    对于Get请求：查询参数在QueryString里保存
    对于Post请求：查询参数在Form表单里保存

    豆瓣

    做爬虫最需要关注的不是页面信息，而是页面信息的数据来源。

    AJAX 方式加载的页面：数据来源一定是JSON

    拿到JSON，就是拿到了网页的数据

    https://movie.douban.com/explore#!type=movie&tag=%E5%8A%A8%E4%BD%9C&sort=recommend&page_limit=20&page_start=0

    "type": "movie",
    "tag": "动作",
    "sort": "recommend,
    "page_limit": "20",
    "page_start": "20"

    人人网
    "Host": "reg.renren.com",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
    "Referer": "http://www.renren.com/",
    "Accept-Encoding": "gzip, deflate",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Cookie": "anonymid=jhbripcz-h5snww; depovince=JX; jebecookies=22a803d5-eca9-43a1-ac06-6be4ea702c37|||||;
    _r01_=1; ick_login=d48b7217-3610-462a-b7b6-a90f053146ab; _de=4ED0D816D96743EA3EAE85F9536018E0;
    p=bbea6aeb59d750f822fbe0c40ec4472f5; first_login_flag=1;
    ln_uact=13818090781; ln_hurl=http://hdn.xnimg.cn/photos/hdn521/20150727/1135/main_6FH7_725c0000155f195a.jpg;
    t=66aec342793499cf8ec71187570c0f475; societyguester=66aec342793499cf8ec71187570c0f475; id=470137425; xnsid=8d6a684e;
    loginfrom=syshome; ch_id=10016; jebe_key=c06488b2-9607-4fdb-ad0c-d8dec5272801%7C09ba35fcdded49efb0f24861edd6619f%7C1526635798880%7C1%7C1526635800465;
    wp_fold=0"



